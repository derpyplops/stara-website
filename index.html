<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Engineering Camp for Alignment Practitioners (RECAP)</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Research Engineering Camp for Alignment Practitioners <span class="accent">(RECAP)</span></h1>
            <p class="subtitle">Singapore • June 2-27, 2025</p>
        </div>
    </header>

    <main>
        <section class="hero">
            <div class="container">
                <div class="hero-content">
                    <div class="hero-image">
                        <img src="images/robot.png" alt="Robot illustration" class="robot-image">
                    </div>
                    <div class="hero-text">
                        <h2>Are you ready to align the AI?</h2>
                        <p>This June (tentatively June 2-27), we're running the Research Engineering Camp for Alignment Practitioners (RECAP) based on the global <a href="https://www.arena.education/" target="_blank">ARENA</a> program — a hands-on, fast-paced accelerator that's launched people into OpenAI, Anthropic, and top safety research fellowships like the <a href="https://www.matsprogram.org/" target="_blank">Machine Alignment Theory Scholars (MATS) Program</a>.</p>
                        <a href="https://airtable.com/appLJFFCH1iF1CZjU/paguYCsFvorWVHoNt/form" class="cta-button">Express Interest <i class="fas fa-arrow-right"></i></a>
                    </div>
                </div>
            </div>
        </section>

        <section class="about">
            <div class="container">
                <p>We're building AI systems we don't yet understand — let alone control. As models grow more capable, the gap between what they can do and what we can guarantee keeps widening. The field of AI safety exists because right now, no one knows how to reliably align powerful AI with human intent. <strong>And we may not get many tries.</strong></p>
                <p>That's why we're running RECAP — a 4-week program that's designed to equip you with the engineering skills to build safe and reliable AI systems.</p>
            </div>
        </section>

        <section class="curriculum">
            <div class="container">
                <h2>Program Curriculum</h2>
                <div class="curriculum-pathway">
                    <div class="curriculum-item">
                        <div class="icon"><i class="fas fa-code"></i></div>
                        <span class="step">Step 1</span>
                        <h3>Fundamentals</h3>
                        <p>Mathematics, programming, and basic neural networks</p>
                        <div class="connector"></div>
                    </div>
                    <div class="curriculum-item">
                        <div class="icon"><i class="fas fa-brain"></i></div>
                        <span class="step">Step 2</span>
                        <h3>Mechanistic Interpretability</h3>
                        <p>Build a transformer from scratch</p>
                        <div class="connector"></div>
                    </div>
                    <div class="curriculum-item">
                        <div class="icon"><i class="fas fa-chart-line"></i></div>
                        <span class="step">Step 3</span>
                        <h3>LLM Evaluations</h3>
                        <p>Learn to properly evaluate large language models</p>
                        <div class="connector"></div>
                    </div>
                    <div class="curriculum-item">
                        <div class="icon"><i class="fas fa-project-diagram"></i></div>
                        <span class="step">Step 4</span>
                        <h3>Capstone Project</h3>
                        <p>Apply your skills to a real AI safety problem</p>
                        <div class="connector"></div>
                    </div>
                </div>
            </div>
        </section>

        <section class="highlight">
            <div class="container">
                <h2>More than another generic AI course, we're focusing on what might be the defining technical challenge of this century.</h2>
                <p class="note">(No prior AI safety experience needed.)</p>
            </div>
        </section>

        <section class="team">
            <div class="container">
                <h2>Our Team</h2>
                <div class="team-grid">
                    <div class="team-member">
                        <h3>Jonathan Ng</h3>
                        <p class="role">Director</p>
                        <ul>
                            <li>Organized AI Safety fellowships at NUS and NTU</li>
                            <li>Graduated from the pilot version of ARENA</li>
                            <li>MATS Spring '23 alumnus</li>
                            <li>Authored MACHIAVELLI, cybercapabilities.org</li>
                        </ul>
                    </div>
                    <div class="team-member">
                        <h3>Clement Neo</h3>
                        <p class="role">Technical Lead / Head Teacher</p>
                        <ul>
                            <li>Founding Research Manager at APART Research</li>
                            <li>Research Assistant at APART Research, mentoring and advising people pivoting into mechanistic interpretability</li>
                            <li>TA'd a similar program in 2023, with alumni now working at AI Safety Camp, Apollo Research, UK AI Safety Institute, US Congress, and 80,000 Hours</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <section class="cta">
            <div class="container">
                <h2>Interested in joining?</h2>
                <p>We're gauging interest now. Takes 5 mins to fill out this form.</p>
                <a href="https://airtable.com/appLJFFCH1iF1CZjU/paguYCsFvorWVHoNt/form" class="cta-button">Express Interest <i class="fas fa-arrow-right"></i></a>
            </div>
        </section>

        <section class="logistics">
            <div class="container">
                <h2>Logistics</h2>
                <p>The bootcamp will be held in-person, based at the Singapore AI Safety Hub (SASH). <a href="https://www.aisafety.sg/" target="_blank">Learn more about SASH</a>.</p>
                <p>Lunch and snacks will also all be included.</p>
            </div>
        </section>

        <section class="benefits">
            <div class="container">
                <h2>What Do Participants Get?</h2>
                <p>Participants will not only strengthen their machine learning engineering foundations, but also develop broader software engineering skills — like organizing codebases effectively and adopting clean, scalable coding practices. These skills are highly relevant for technical roles in AI safety, whether at dedicated orgs like Apollo, FAR, and METR, or on alignment teams at frontier labs like Anthropic.</p>
                <p>By the end of the bootcamp, each participant will have built a personal GitHub portfolio showcasing the projects they've completed. This can be a strong asset when applying to technical AI safety positions or internships.</p>
                <p>We also expect that working alongside other alignment-focused teams and researchers will naturally lead to valuable peer exchange, idea-sharing, and potential collaboration.</p>
            </div>
        </section>

        <section class="contact">
            <div class="container">
                <h2>Contact Us</h2>
                <p>Get in touch: jonathan.ng1 at gmail</p>
            </div>
        </section>
    </main>

    <script src="script.js"></script>
</body>
</html> 